{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import npy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load pre-trained word embeddings\n",
    "word_vectors = KeyedVectors.load_word2vec_format('path/to/pretrained/word2vec.bin', binary=True)\n",
    "\n",
    "# Load IMDB reviews dataset\n",
    "df = pd.read_csv(\"path/to/imdb_reviews.csv\")\n",
    "\n",
    "# Split data into features and labels\n",
    "X = df['review']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Convert labels to binary values (0 or 1)\n",
    "y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, y)))\n",
    "\n",
    "# Tokenize the reviews\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(n_words=5000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "# Convert the text data to sequence\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Pad sequences to same length\n",
    "max_words = 500\n",
    "X = sequence.pad_sequences(X, maxlen=max_words)\n",
    "\n",
    "# Replace the word index with pre-trained word embeddings\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word_vectors:\n",
    "        embedding_matrix[i] = word_vectors[word]\n",
    "\n",
    "# Define model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, 300, weights=[embedding_matrix], input_length=max_words, trainable=False))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Use early stopping to prevent overfitting\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "# Train the model\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, epochs=n_epochs, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on test data\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test accuracy:', scores[1])\n",
    "\n",
    "# Define the path to save the model\n",
    "model_path = '/content/drive/My Drive/my_model.h5'\n",
    "\n",
    "# Save the model\n",
    "model.save(model_path)\n",
    "\n",
    "# Load the model\n",
    "loaded_model = keras.models.load_model(model_path)\n",
    "\n",
    "# Define a function to predict sentiment\n",
    "def predict_sentiment(review):\n",
    "    # Tokenize and pad the review\n",
    "    review_seq = tokenizer.texts_to_sequences([review])\n",
    "    review_seq = sequence.pad_sequences(review_seq, maxlen=max_words)\n",
    "    # Predict the sentiment using the loaded model\n",
    "    prediction = loaded_model.predict(review_seq)[0\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
